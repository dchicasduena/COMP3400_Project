{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP 3400: Data Preparation Techniques Project\n",
    "## Analysis of popular movies from 1980 to 2019\n",
    "\n",
    "**Group Members:**\n",
    "  - Liudmila Strelnikova 201819885\n",
    "  - David Chicas 201919354\n",
    "  \n",
    " ## Part One: Changes From Iteration One\n",
    "\n",
    "In this project submission you can find a PDF that addresses all the instrutor's comments about the changes we can make to Iteration 1 to improve it. Those changes have already been addressed in this iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import jinja2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "pd.options.mode.chained_assignment = None #chained assignments are not flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies2 = pd.read_csv(\"movies2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies2.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: Data Scaling Pre-Assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = movies2[['score', 'runtime', 'gross']].dropna()\n",
    "ndf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function plots graphs in a grid whose dimentions are 2 x (number of attributes).\n",
    "#A row in the grid consits of two sets of axes. The first set of axes has a histogram of an numeric\n",
    "#attribute, along with its density function. The second set of axes consists of a histogram\n",
    "#and density plot of a normalized version of the attribute. The method of normalization is set outsite\n",
    "#the function.\n",
    "\n",
    "# @df is the dataframe with numeric attributes \n",
    "# @columns is an array with the names of columns in the df\n",
    "# @df_transformed is a dataframe with normalized attributes of df\n",
    "# @fit_title is the overall title of all the plots\n",
    "\n",
    "def density_plots(df, columns, df_transformed, fig_title, constrained_layout=True):\n",
    "    fig, axs = plt.subplots(len(columns),2,figsize=(16,18))\n",
    "    fig.suptitle(fig_title, fontsize=16)\n",
    "    axs = axs.flatten()\n",
    "    i = 0 \n",
    "    for c in columns:\n",
    "        ndf[c].hist(ax=axs[i], density=True) # normalizes the density\n",
    "        ndf[c].plot.density(ax=axs[i], title=c)\n",
    "        df_transformed[c].hist(ax=axs[i+1], density=True, stacked=True) # normalizes the density\n",
    "        df_transformed[c].plot.density(ax=axs[i+1], title=str(c+'_transformed'))\n",
    "        i+=2\n",
    "\n",
    "val_cols_names = ['score','runtime','gross']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "min_max_tdf = scaler.fit_transform(ndf)\n",
    "min_max_tdf = pd.DataFrame(min_max_tdf, index=ndf.index, columns=ndf.columns)\n",
    "\n",
    "density_plots(ndf, val_cols_names, min_max_tdf, 'MinMax Scaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()\n",
    "maxabs_tdf = scaler.fit_transform(ndf)\n",
    "maxabs_tdf = pd.DataFrame(maxabs_tdf, index=ndf.index, columns=ndf.columns)\n",
    "\n",
    "density_plots(ndf, val_cols_names, maxabs_tdf, 'MaxAbs Scaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "robust_tdf = scaler.fit_transform(ndf)\n",
    "robust_tdf = pd.DataFrame(robust_tdf, index=ndf.index, columns=ndf.columns)\n",
    "\n",
    "density_plots(ndf, val_cols_names, robust_tdf, 'Robust Scaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# scaler = QuantileTransformer(n_quantiles=10, random_state=0)\n",
    "scaler = QuantileTransformer(n_quantiles=10, random_state=0, output_distribution='normal')\n",
    "qtl_tdf = scaler.fit_transform(ndf)\n",
    "qtl_tdf = pd.DataFrame(qtl_tdf, index=ndf.index, columns=ndf.columns)\n",
    "\n",
    "density_plots(ndf, val_cols_names, qtl_tdf, 'Quantile Scaler')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler = Normalizer()\n",
    "zscore_tdf = scaler.fit_transform(ndf)\n",
    "zscore_tdf = pd.DataFrame(zscore_tdf, index=ndf.index, columns=ndf.columns)\n",
    "\n",
    "density_plots(ndf, val_cols_names, zscore_tdf, 'Z-score Scaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "\n",
    "scaler = FunctionTransformer(np.log1p)\n",
    "log_tdf = scaler.fit_transform(ndf)\n",
    "log_tdf = pd.DataFrame(log_tdf, index=ndf.index, columns=ndf.columns)\n",
    "\n",
    "density_plots(ndf, val_cols_names, log_tdf, 'Log Scaler')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot all the normalization and standardization plots togethe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms_density_for_scalers(df, columns, fig_title, scalers, scaler_names):\n",
    "    fig, axs = plt.subplots(len(columns),len(scalers)+1,figsize=(16,18),constrained_layout=True)\n",
    "    fig.suptitle(fig_title, fontsize=16)\n",
    "    axs = axs.flatten()\n",
    "    i = 0 \n",
    "    for c in columns:\n",
    "        ndf[c].hist(ax=axs[i], density=True) # normalizes the density\n",
    "        ndf[c].plot.density(ax=axs[i], title=c)\n",
    "        i+=1\n",
    "        for j in range(len(scalers)):\n",
    "            df_transformed = scalers[j].fit_transform(df)\n",
    "            df_transformed = pd.DataFrame(df_transformed, index=df.index, columns=df.columns)\n",
    "            df_transformed[c].hist(ax=axs[i], density=True, stacked=True) # normalizes the density\n",
    "            df_transformed[c].plot.density(ax=axs[i], title=scaler_names[j])\n",
    "            i+=1\n",
    "            \n",
    "\n",
    "val_cols_names = ['score','runtime','gross']\n",
    "scaler_names = ['MinMax', 'Z-score', 'MaxAbs', 'Robust', 'Quantile', 'Log']\n",
    "arr_scalers = [MinMaxScaler(), Normalizer(), MaxAbsScaler(), RobustScaler(), QuantileTransformer(n_quantiles=10, random_state=0, output_distribution='normal'),FunctionTransformer(np.log1p)]\n",
    "plot_histograms_density_for_scalers(ndf, val_cols_names, 'Scalers effect on data', arr_scalers, scaler_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the charts above, it is clear that the only normalization technicque that worked on the gross attribute is quantile transform scaler (QTS). QTS maps the variable's probability distribution to another probablility distribution, and performs well with unimodal data, which includes the distribution of gross values.\n",
    "\n",
    "Score has a normal distribution with minimal skewness, so min-max scaler works well for this attribute.\n",
    "\n",
    "Runtime is a bit more skewed than score, so robust scaler is the best choice for this attribute's normalization. Robust scaler removes the median, and scales data according to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
