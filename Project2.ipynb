{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7485a0c-de38-41bb-becb-0d2b310dbcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import jinja2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "pd.options.mode.chained_assignment = None #chained assignments are not flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091ab227-ea82-4eeb-98ad-90da44375af0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'movies2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m movies2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovies2.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'movies2.csv'"
     ]
    }
   ],
   "source": [
    "movies2 = pd.read_csv(\"movies2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae606d5b-0bf1-4109-bd85-99d9d0ac0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31decac-75a8-4f47-b189-7186039154bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies2.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f904ab-9c7e-4db6-a25e-46833dfb9538",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984b628-098a-44b9-881a-4b5c92cfbfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = movies2[['score', 'runtime', 'gross']].dropna()\n",
    "ndf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7192b-795b-4dc0-bf89-1f3c5dc85b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function plots graphs in a grid whose dimentions are 2 x (number of attributes).\n",
    "#A row in the grid consits of two sets of axes. The first set of axes has a histogram of an numeric\n",
    "#attribute, along with its density function. The second set of axes consists of a histogram\n",
    "#and density plot of a normalized version of the attribute. The method of normalization is set outsite\n",
    "#the function.\n",
    "\n",
    "# @df is the dataframe with numeric attributes \n",
    "# @columns is an array with the names of columns in the df\n",
    "# @df_transformed is a dataframe with normalized attributes of df\n",
    "# @fit_title is the overall title of all the plots\n",
    "\n",
    "def density_plots(df, columns, df_transformed, fig_title, constrained_layout=True):\n",
    "    fig, axs = plt.subplots(len(columns),2,figsize=(16,18))\n",
    "    fig.suptitle(fig_title, fontsize=16)\n",
    "    axs = axs.flatten()\n",
    "    i = 0 \n",
    "    for c in columns:\n",
    "        ndf[c].hist(ax=axs[i], density=True) # normalizes the density\n",
    "        ndf[c].plot.density(ax=axs[i], title=c)\n",
    "        df_transformed[c].hist(ax=axs[i+1], density=True, stacked=True) # normalizes the density\n",
    "        df_transformed[c].plot.density(ax=axs[i+1], title=str(c+'_transformed'))\n",
    "        i+=2\n",
    "\n",
    "val_cols_names = ['score','runtime','gross']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "min_max_tdf = scaler.fit_transform(ndf)\n",
    "min_max_tdf = pd.DataFrame(min_max_tdf, index=ndf.index, columns=ndf.columns)\n",
    "\n",
    "density_plots(ndf, val_cols_names, min_max_tdf, 'MinMax Scaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ec783-323e-4ed6-9685-40f307b11ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()\n",
    "maxabs_tdf = scaler.fit_transform(ndf)\n",
    "maxabs_tdf = pd.DataFrame(maxabs_tdf, index=ndf.index, columns=ndf.columns)\n",
    "\n",
    "density_plots(ndf, val_cols_names, maxabs_tdf, 'MaxAbs Scaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0007c6-8cf3-4f0c-8d81-7cdbedc8dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "robust_tdf = scaler.fit_transform(ndf)\n",
    "robust_tdf = pd.DataFrame(robust_tdf, index=ndf.index, columns=ndf.columns)\n",
    "\n",
    "density_plots(ndf, val_cols_names, robust_tdf, 'Robust Scaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff0179b-b9fe-4a28-b39b-0c4d279ad811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# scaler = QuantileTransformer(n_quantiles=10, random_state=0)\n",
    "scaler = QuantileTransformer(n_quantiles=10, random_state=0, output_distribution='normal')\n",
    "qtl_tdf = scaler.fit_transform(ndf)\n",
    "qtl_tdf = pd.DataFrame(qtl_tdf, index=ndf.index, columns=ndf.columns)\n",
    "\n",
    "density_plots(ndf, val_cols_names, qtl_tdf, 'Quantile Scaler')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fbb14e-a131-434b-ae27-12590ccf16de",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e702c1-7701-4d91-aacf-5b4a856e6d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler = Normalizer()\n",
    "zscore_tdf = scaler.fit_transform(ndf)\n",
    "zscore_tdf = pd.DataFrame(zscore_tdf, index=ndf.index, columns=ndf.columns)\n",
    "\n",
    "density_plots(ndf, val_cols_names, zscore_tdf, 'Z-score Scaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53541c-d7b9-4396-b765-de1c875b09a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "\n",
    "scaler = FunctionTransformer(np.log1p)\n",
    "log_tdf = scaler.fit_transform(ndf)\n",
    "log_tdf = pd.DataFrame(log_tdf, index=ndf.index, columns=ndf.columns)\n",
    "\n",
    "density_plots(ndf, val_cols_names, log_tdf, 'Log Scaler')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c21f40-144a-44ca-9737-9b9926a6a9c1",
   "metadata": {},
   "source": [
    "We can plot all the normalization and standardization plots togethe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f39506-90fd-4ef0-8363-859ce1cefbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms_density_for_scalers(df, columns, fig_title, scalers, scaler_names):\n",
    "    fig, axs = plt.subplots(len(columns),len(scalers)+1,figsize=(16,18),constrained_layout=True)\n",
    "    fig.suptitle(fig_title, fontsize=16)\n",
    "    axs = axs.flatten()\n",
    "    i = 0 \n",
    "    for c in columns:\n",
    "        ndf[c].hist(ax=axs[i], density=True) # normalizes the density\n",
    "        ndf[c].plot.density(ax=axs[i], title=c)\n",
    "        i+=1\n",
    "        for j in range(len(scalers)):\n",
    "            df_transformed = scalers[j].fit_transform(df)\n",
    "            df_transformed = pd.DataFrame(df_transformed, index=df.index, columns=df.columns)\n",
    "            df_transformed[c].hist(ax=axs[i], density=True, stacked=True) # normalizes the density\n",
    "            df_transformed[c].plot.density(ax=axs[i], title=scaler_names[j])\n",
    "            i+=1\n",
    "            \n",
    "\n",
    "val_cols_names = ['score','runtime','gross']\n",
    "scaler_names = ['MinMax', 'Z-score', 'MaxAbs', 'Robust', 'Quantile', 'Log']\n",
    "arr_scalers = [MinMaxScaler(), Normalizer(), MaxAbsScaler(), RobustScaler(), QuantileTransformer(n_quantiles=10, random_state=0, output_distribution='normal'),FunctionTransformer(np.log1p)]\n",
    "plot_histograms_density_for_scalers(ndf, val_cols_names, 'Scalers effect on data', arr_scalers, scaler_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cc48ee-d624-4b48-885c-67d85905b3e1",
   "metadata": {},
   "source": [
    "From the charts above, it is clear that the only normalization technicque that worked on the gross attribute is quantile transform scaler (QTS). QTS maps the variable's probability distribution to another probablility distribution, and performs well with unimodal data, which includes the distribution of gross values.\n",
    "\n",
    "Score has a normal distribution with minimal skewness, so min-max or max-abs scalers work well for this attribute.\n",
    "\n",
    "Runtime is a bit more skewed than score, so robust scaler is the best choice for this attribute's normalization. Robust scaler removes the median, and scales data according to inter-quartile range (IQR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80f913-42cf-48bf-9848-35d94deed364",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53037386-9e74-4702-bdc0-8ec52615ff42",
   "metadata": {},
   "source": [
    "Unsupervised learning models only use continuous attributes, therefore attributes rating, genre, and  country should be encoded. \n",
    "The attributes do not possess much hierarchy, and can be encoded with a simple label encoder.\n",
    "\n",
    "Note: I have to make a translation for encodings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d21ecd6-99e3-4a94-a5e2-25a41d8b0291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder() \n",
    "df_le = movies2.copy()\n",
    "\n",
    "df_le['rating_en'] = le.fit_transform(df_le['rating'])\n",
    "df_le['genre_en'] = le.fit_transform(df_le['genre'])\n",
    "df_le['country_en'] = le.fit_transform(df_le['country'])\n",
    "\n",
    "to_drop = ['rating', 'genre', 'country']\n",
    "df_le = df_le.drop(to_drop,axis=1)\n",
    "\n",
    "df_le = df_le.rename(columns = {\"rating_en\":\"rating\", \"genre_en\":\"genre\", \"country_en\":\"country\"})\n",
    "movies2 = df_le\n",
    "movies2.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
